% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/eval_model.R
\name{eval_model}
\alias{eval_model}
\title{Evaluate results for images subject to manual review}
\usage{
eval_model(
  preds = NULL,
  data = NULL,
  filepath = NULL,
  true_class = NULL,
  assess_counts = FALSE,
  true_count = NULL,
  event_level = "image",
  seq_id = NULL
)
}
\arguments{
\item{preds}{df of output from deploy_model. Currently only supports 'long' prediction format}

\item{data}{ground truth results. Must be an R dataframe and contain the following columns:}

\item{filepath}{name of the column in `data` that contains absolute paths to the the images being assessed}

\item{true_class}{name of the column in `data` that contains ground truth classifications. 
Must match the class level of `preds` i.e. if `preds` contained predictions for the species model, this
column must contain ground truth species.}

\item{assess_counts}{boolean. Evaluate how well the model predicts counts in addition to classes. Default = FALSE.}

\item{true_count}{name of the column in `data` that contains ground truth counts for the class listed in 
`true_class` for a given row.}

\item{event_level}{evaluate results at the individual level or at the sequence level? 
Accepts values `c("image", "sequence")`. If including evaluation by sequence, @param seq_id must be specified}

\item{seq_id}{name of the column in `data` that contains sequence ids for each image. If you need to generate this
information, see the *generate_sequences* function.}
}
\description{
join model_predictions output to a user-generated df of ground 
truth identifications and assess overall and/or classwise performance
}
\details{
this function takes your output results from deploy_model and joins them
to a dataframe with ground truth identifications. The final object returned is a list containing 
overall and class-wise evaluation metrics for class identification at the image and/or
sequence level, and optionally for counts at the image and/or sequence level.
}
