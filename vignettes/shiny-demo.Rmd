---
title: "Shiny Demo"
output: 
  rmarkdown::html_vignette:
    toc: true
    self_contained: true
vignette: >
  %\VignetteIndexEntry{Shiny Demo}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

```
## Before you start  
  
### Who is this for?   
  
Anyone new to using CameraTrapDetectoR can benefit from this detailed example, even experienced R users. Users who are less comfortable with coding in R or RStudio may find the Shiny app interface more efficient than running the model directly in the R console. We provide the printout of your coded arguments inside the Shiny app to give you that option if desired.  
  
This document will be extremely helpful using the CameraTrapDetectoR desktop application. The interface is exactly the same as the Shiny application, without any R coding to open the application. Please see the section on running the Desktop app in our user manual for install instructions.  
  
### Sample Images  
We've provided five test [images](https://github.com/CameraTrapDetectoR/CameraTrapDetectoR/blob/main/inst/extdata/test_images.zip) you can use to follow along with the demo and test out different model arguments. Download and extract the images, save them to an easily accessible directory, and proceed to the [demo](#demo).     
  

### Organize your image directory {#organize}   
  
An organized image directory is essential to making sure your CameraTrapDetectoR session runs smoothly. Place all images to be run through the model in the same directory. You may set up a recursive directory with images in different folders, but make sure to remove any images you do not wish to run through the model. CameraTrapDetectoR only searches for file types specified in the [file_extensions](#fileext) argument; it will ignore all other file types.  
  
The model was trained on images of size 408 x 307 pixels, and will resize your images before running them through the neural network. You may see a slight decrease in model run time if you resize your images to these dimensions before running the model.  

You have the option to enter location data for your images (latitude / longitude) to help narrow down potential species. However, this option can only be set once for all images in a given model run. If you would like to use this option, all your images must originate from the same location. Alternatively, you can split your image directory by location, and run the model separately on images from each location.  

CameraTrapDetectoR desktop app users may proceed directly to the [Arguments](#Arguments) section.    
  
## Open the CameraTrapDetectoR Shiny application {#open} 
  
Once you have [installed](https://github.com/CameraTrapDetectoR/CameraTrapDetectoR) CameraTrapDetectoR, copy + paste the following lines of code in your console:  

```{r setup, eval = FALSE}
library(CameraTrapDetectoR)
runShiny("deploy")
```  
  
You should see a pop-up window with the drop-down arguments on the left, and brief explanations of each argument on the right.  


![](images/startup.PNG)  


The next section will go over each argument in detail.  
  
## Arguments {#Arguments}     
  
### data_dir {#datadir}    
  
```{r datdir, echo=FALSE, out.width='40%', out.extra='style="float:right; padding:10px"'}
knitr::include_graphics("images/datadir_empty.png")
```  
The first argument you need to specify is where on your machine CameraTrapDetectoR should look for your images. Click the **data_dir** button to open your file directory, then use the black arrows to the left of the folders to navigate to your image directory. Once you navigate to the folder that contains all your images, click the text on the left pane to select that folder.  
  
If you have [organized](#organize) your images in multiple folders, select the folder that *contains* all your separate image folders and set the [recursive](#rec) argument to **TRUE** so the model sees all your images. If you wish to only run images in a particular folder or sub-directory, make that selection carefully so you do not send extra images through the model.  
  
![Run all images in Folders 1, 2, 3](images/datadir_images.png){width='45%'}
![Run images just in Folder 1](images/datadir_imfold.png){width='45%'}   
  
### model_type {#model}  
  
This drop-down menu allows you to select which model you would like to run. CameraTrapDetectoR currently supports four different models. The **general** model predicts to the level of mammal, bird, human, and vehicle. The **family** model recognizes 33 unique taxonomic families listed in [Table 3](https://www.biorxiv.org/content/10.1101/2022.02.07.479461v1.full). The **species** model recognizes 75 unique taxonomic species listed in [Table 2] (https://www.biorxiv.org/content/10.1101/2022.02.07.479461v1.full). The **pig_only** model outputs only two labels: *pig*, *not pig*; this model was developed for applications to feral swine research.  

### recursive {#rec}  
  
This **TRUE** / **FALSE** drop-down menu tells the program if it should search for images within all folders inside your selected [data_dir](#datadir). If you have images stored in different folders you wish to send to the model, select **TRUE**.  
  
### file_extensions {#fileext}  
  
```{r files, echo=FALSE, out.width='30%', out.extra='style="float:right; padding:10px"'}
knitr::include_graphics("images/file_extensions.png")
```  
CameraTrapDetectoR currently supports the following file types: *.jpg*, *.png*, *.tif*, and *.pdf*. You can specify these file types by checking or unchecking the boxes on the Arguments menu. The model will ignore any files inside your [**data_dir**](#datadir) that are not one of the accepted file types. More chosen file types does not penalize speed; if you want to guarantee all eligible images are run through the model, check all boxes.  

### make_plots {#plots}  
  
CameraTrapDetectoR has the ability to return copies of your images with boxes around its predictions in real time. If you would like to activate this option, select **TRUE**.  
  
### plot_label  
  
If you choose make prediction [plots](#plots), you also have the option to label your predictions on the image. These labels are determined by the classes of your [model](#model).  
  
### output_dir {#outdir}  
  
This argument determines where your model output is stored. The default argument is to leave it empty, which creates a folder inside your [data_dir](#datadir) named after [model_type](#model) and datetime the model began running.  
     
### sample50  
  
A large image dataset will take a long time to run; you may want to check your model arguments on a smaller sample before committing to a full model run. In this testing process, set this argument to **TRUE**.    
   
### write_bbox_csv  
  
Some users may want the placement and dimensions of predicted bounding boxes. Setting this argument to **TRUE** will return a .csv file in your output directory titled *predicted_bboxes*. This file has a separate row for each prediction with the following fields: full image path; predicted class; confidence in prediction;total predictions per image; and bounding box coordinates. Bounding box coordinates are given with (XMin, YMin) and (XMax, YMax) points corresponding to the upper left and lower right corners of the bounding box, in proportion to image size (i.e. coordinates are in the range [0, 1]).  
The file will contain all model predictions, even those below the chosen [score_threshold](#score).  
    
### score_threshold {#score}  
  
The model provides a confidence score for every prediction, indicating the model's level of confidence in that prediction. CameraTrapDetectoR only reports predictions above the set score threshold. A lower score threshold may capture more true predictions, but may also report more false predictions. A higher score threshold will reduce false predictions, but may also fail to capture some true predictions. An optimal score threshold will depend on your research questions and your images. You may want to run a small sample through the model using different score thresholds to determine what threshold is best for your data. You can either type in your selection between 0 and 0.99, or click the arrows to toggle by increments of 0.01.  
  
### overlap_correction  
  
It is possible that predicted bounding boxes on the same image may overlap. Overlapping bounding boxes may be due to the presence of multiple individuals occupying close space. Alternatively, overlapping bounding boxes may be caused by multiple predictions of the same individual. If you wish to assess overlapping boxes and combine boxes that overlap into a single prediction, set this argument to **TRUE** and choose a reasonable [overlap_threshold](#overlap_thresh). Using this feature depends on the context of your research question and your data.     
  
### overlap_threshold {#overlap_thresh}  
  
If you use an overlap correction, this argument sets the proportion of overlapping area of two boxes for them to be returned as a single detection.   

### return_data_frame  

### prediction_format  
  
### latitude  

### longitude  

### h  

### w  

### lty  
  
### lwd  
  
### col  
 
  

